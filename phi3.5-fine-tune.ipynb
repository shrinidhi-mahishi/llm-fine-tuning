{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Library Installation","metadata":{}},{"cell_type":"code","source":"!pip install unsloth==2025.5.5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:39:19.910487Z","iopub.execute_input":"2025-05-28T09:39:19.910746Z","iopub.status.idle":"2025-05-28T09:42:18.599179Z","shell.execute_reply.started":"2025-05-28T09:39:19.910727Z","shell.execute_reply":"2025-05-28T09:42:18.598468Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth==2025.5.5\n  Downloading unsloth-2025.5.5-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2025.5.7 (from unsloth==2025.5.5)\n  Downloading unsloth_zoo-2025.5.8-py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (2.6.0+cu124)\nCollecting xformers>=0.0.27.post2 (from unsloth==2025.5.5)\n  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth==2025.5.5)\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (3.2.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (25.0)\nCollecting tyro (from unsloth==2025.5.5)\n  Downloading tyro-0.9.22-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers!=4.47.0,==4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (4.51.3)\nRequirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (3.6.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (1.5.2)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth==2025.5.5)\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.14.0)\nRequirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.31.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.32.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth==2025.5.5) (0.21.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (3.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.5) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.5) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.5) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.5) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth==2025.5.5) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth==2025.5.5) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth==2025.5.5) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.5) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.5) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.5) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.5) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.5) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth==2025.5.5) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth==2025.5.5) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth==2025.5.5) (1.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.5) (14.0.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.5.7->unsloth==2025.5.5)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.7->unsloth==2025.5.5) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.5.7->unsloth==2025.5.5)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting torch>=2.4.0 (from unsloth==2025.5.5)\n  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\nCollecting sympy>=1.13.3 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth==2025.5.5)\n  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\nCollecting triton>=3.0.0 (from unsloth==2025.5.5)\n  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth==2025.5.5) (75.2.0)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth==2025.5.5) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth==2025.5.5)\n  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth==2025.5.5) (0.16)\nCollecting shtab>=1.5.6 (from tyro->unsloth==2025.5.5)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth==2025.5.5) (4.4.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (3.11.18)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.47.0,==4.51.3->unsloth==2025.5.5) (2025.4.26)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.5) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.5) (2.19.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth==2025.5.5) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth==2025.5.5) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth==2025.5.5) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth==2025.5.5) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth==2025.5.5) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth==2025.5.5) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.5) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.5) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth==2025.5.5) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth==2025.5.5) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth==2025.5.5) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth==2025.5.5) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth==2025.5.5) (1.17.0)\nDownloading unsloth-2025.5.5-py3-none-any.whl (265 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.3/265.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.5.8-py3-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.9.22-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, torch, cut_cross_entropy, trl, xformers, unsloth_zoo, torchvision, bitsandbytes, unsloth\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.22 unsloth-2025.5.5 unsloth_zoo-2025.5.8 xformers-0.0.30\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Standard library\nimport os\n\n# Third-party libraries\nimport datasets\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom unsloth import FastLanguageModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:42:18.600743Z","iopub.execute_input":"2025-05-28T09:42:18.601272Z","iopub.status.idle":"2025-05-28T09:42:50.645813Z","shell.execute_reply.started":"2025-05-28T09:42:18.601247Z","shell.execute_reply":"2025-05-28T09:42:50.645036Z"}},"outputs":[{"name":"stderr","text":"2025-05-28 09:42:30.263154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748425350.433143      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748425350.481394      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_35/1279870000.py:11: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n\nPlease restructure your imports with 'import unsloth' at the top of your file.\n  from unsloth import FastLanguageModel\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Parameters","metadata":{}},{"cell_type":"code","source":"max_seq_length = 512 # Maximum number of tokens per input sequence\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:42:50.646540Z","iopub.execute_input":"2025-05-28T09:42:50.647154Z","iopub.status.idle":"2025-05-28T09:42:50.651556Z","shell.execute_reply.started":"2025-05-28T09:42:50.647135Z","shell.execute_reply":"2025-05-28T09:42:50.650894Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"microsoft/Phi-3.5-mini-instruct\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:42:50.653131Z","iopub.execute_input":"2025-05-28T09:42:50.653351Z","iopub.status.idle":"2025-05-28T09:43:07.127162Z","shell.execute_reply.started":"2025-05-28T09:42:50.653333Z","shell.execute_reply":"2025-05-28T09:43:07.126559Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.5.5: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6f86bf491843e0a41defce4bd2ce27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/140 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e356ab1f22024837904a2270d5c7254a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67efe89512c548f7a9b4d17530c2b27a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c891e79638864cf1b02cdea00eee0da3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91695d286ad1410e9deda0f1de492f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f48b0e8af164c54bb2a11cd20dbe5f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594685a3518f4e48988e7d00cd398c68"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:07.127874Z","iopub.execute_input":"2025-05-28T09:43:07.128093Z","iopub.status.idle":"2025-05-28T09:43:07.134519Z","shell.execute_reply.started":"2025-05-28T09:43:07.128075Z","shell.execute_reply":"2025-05-28T09:43:07.133750Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer.padding_side = 'right' # Set padding to the right side for tokenized inputs\nEOS_TOKEN = tokenizer.eos_token\nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:07.135364Z","iopub.execute_input":"2025-05-28T09:43:07.135751Z","iopub.status.idle":"2025-05-28T09:43:07.420932Z","shell.execute_reply.started":"2025-05-28T09:43:07.135725Z","shell.execute_reply":"2025-05-28T09:43:07.420051Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Preparing the model for PEFT","metadata":{}},{"cell_type":"code","source":"# wraps the base model with LoRA config\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = True,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:07.421928Z","iopub.execute_input":"2025-05-28T09:43:07.422307Z","iopub.status.idle":"2025-05-28T09:43:15.390132Z","shell.execute_reply.started":"2025-05-28T09:43:07.422283Z","shell.execute_reply":"2025-05-28T09:43:15.389400Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.5.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:15.391082Z","iopub.execute_input":"2025-05-28T09:43:15.391376Z","iopub.status.idle":"2025-05-28T09:43:15.402079Z","shell.execute_reply.started":"2025-05-28T09:43:15.391352Z","shell.execute_reply":"2025-05-28T09:43:15.401367Z"}},"outputs":[{"name":"stdout","text":"trainable params: 59,768,832 || all params: 3,880,848,384 || trainable%: 1.5401\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Training Dataset","metadata":{}},{"cell_type":"code","source":"import random\n\n# List of 50 NSE-listed companies\ncompanies = [\n    \"TCS\", \"Reliance Industries\", \"HDFC Bank\", \"Infosys\", \"ICICI Bank\", \"HUL\", \"Bajaj Finance\", \n    \"Kotak Mahindra Bank\", \"SBI\", \"Axis Bank\", \"Bharti Airtel\", \"Asian Paints\", \"Maruti Suzuki\", \n    \"Tata Motors\", \"Wipro\", \"Larsen & Toubro\", \"ITC\", \"HCL Technologies\", \"Nestle India\", \n    \"Sun Pharma\", \"Adani Enterprises\", \"Titan Company\", \"Mahindra & Mahindra\", \"Dr Reddy's Labs\", \n    \"UltraTech Cement\", \"JSW Steel\", \"Power Grid\", \"Tech Mahindra\", \"Dabur India\", \"Pidilite Industries\", \n    \"Godrej Consumer\", \"Bajaj Auto\", \"Eicher Motors\", \"Cipla\", \"Shree Cement\", \"Tata Steel\", \n    \"Hindalco Industries\", \"Grasim Industries\", \"Bharat Petroleum\", \"ONGC\", \"NTPC\", \n    \"Adani Ports\", \"Coal India\", \"Havells India\", \"Berger Paints\", \"Divi's Laboratories\", \n    \"Britannia Industries\", \"UPL\", \"Ambuja Cements\", \"Hero MotoCorp\", \"Zee Entertainment\"\n]\n\n# Financial terms and corresponding Solr fields\nquery_types = [\n    (\"results\", \"report_type:results\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]),\n    (\"annual report\", \"report_type:annual_report\", []),\n    (\"10K\", \"report_type:10K\", []),\n    (\"revenue\", \"financial_metric:revenue\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"\"]),\n    (\"earnings per share\", \"financial_metric:eps\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"\"]),\n    (\"profit\", \"financial_metric:profit\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"\"]),\n    (\"balance sheet\", \"report_type:balance_sheet\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"\"]),\n    (\"cash flow\", \"report_type:cash_flow\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"\"])\n]\n\nyears = [2020, 2021, 2022, 2023, 2024, 2025]\n\n# Templates\nnatural_templates = [\n    \"Show me {query_type} of {company} for {year}\",\n    \"What is the {query_type} for {company} in {year}?\",\n    \"Give me {company} {query_type} for {year}\",\n    \"{company} {query_type} {year}\",\n    \"Find {query_type} of {company} in {year}\",\n    \"Show {company}'s {query_type} for {quarter} {year}\",\n    \"What are {company}'s {query_type} for {quarter} {year}?\",\n    \"Give me {quarter} {year} {query_type} for {company}\"\n]\n\nnatural_queries = []\nsolr_queries = []\n\nfor _ in range(1000):\n    company = random.choice(companies)\n    year = random.choice(years)\n    query_type, solr_field, quarters = random.choice(query_types)\n    quarter = random.choice(quarters) if quarters else \"\"\n\n    # Generate natural query\n    if quarter:\n        template = random.choice([t for t in natural_templates if \"{quarter}\" in t])\n        natural_query = template.format(company=company, query_type=query_type, year=year, quarter=quarter)\n    else:\n        template = random.choice([t for t in natural_templates if \"{quarter}\" not in t])\n        natural_query = template.format(company=company, query_type=query_type, year=year)\n\n    # Generate Solr query\n    solr_parts = [f'company_name:\"{company}\"', f'fiscal_year:{year}']\n    if quarter:\n        solr_parts.append(f'fiscal_quarter:\"{quarter}\"')\n    solr_parts.append(solr_field)\n    solr_query = \" AND \".join(solr_parts)\n\n    natural_queries.append(natural_query)\n    solr_queries.append(solr_query)\n\nprint(\"Sample Natural Queries and Solr Queries:\")\nfor i in range(5):\n    print(f\"Natural Query: {natural_queries[i]}\")\n    print(f\"Solr Query: {solr_queries[i]}\\n\")\n\nprint(f\"Total rows: {len(natural_queries)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:15.402783Z","iopub.execute_input":"2025-05-28T09:43:15.402965Z","iopub.status.idle":"2025-05-28T09:43:19.778909Z","shell.execute_reply.started":"2025-05-28T09:43:15.402951Z","shell.execute_reply":"2025-05-28T09:43:19.778120Z"}},"outputs":[{"name":"stdout","text":"Sample Natural Queries and Solr Queries:\nNatural Query: Give me Infosys 10K for 2023\nSolr Query: company_name:\"Infosys\" AND fiscal_year:2023 AND report_type:10K\n\nNatural Query: Give me Tata Motors earnings per share for 2023\nSolr Query: company_name:\"Tata Motors\" AND fiscal_year:2023 AND financial_metric:eps\n\nNatural Query: What are Larsen & Toubro's balance sheet for Q3 2023?\nSolr Query: company_name:\"Larsen & Toubro\" AND fiscal_year:2023 AND fiscal_quarter:\"Q3\" AND report_type:balance_sheet\n\nNatural Query: What are Wipro's earnings per share for Q2 2020?\nSolr Query: company_name:\"Wipro\" AND fiscal_year:2020 AND fiscal_quarter:\"Q2\" AND financial_metric:eps\n\nNatural Query: Show me annual report of Hindalco Industries for 2025\nSolr Query: company_name:\"Hindalco Industries\" AND fiscal_year:2025 AND report_type:annual_report\n\nTotal rows: 1000\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"df = pd.DataFrame({\n    \"natural_lang_queries\": natural_queries,\n    \"solr_queries\": solr_queries\n})\ncsv_path = \"train_data_nse.csv\"\ndf.to_csv(csv_path, index=False)\n\ndataset = load_dataset('csv', data_files=csv_path, split=\"train\")\ndataset = dataset.shuffle(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:19.781531Z","iopub.execute_input":"2025-05-28T09:43:19.782037Z","iopub.status.idle":"2025-05-28T09:43:20.125772Z","shell.execute_reply.started":"2025-05-28T09:43:19.782006Z","shell.execute_reply":"2025-05-28T09:43:20.125030Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f83d77e03fc422e8eab8f8cfb6e4865"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"query_rewrite_prompt = \"\"\" \nTask: Rewrite the natural language queries about company financial performance into concise, search-engine-friendly Solr queries.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\n\n\"\"\"\n\ninstruction_to_llm = \"\"\"\n1-Use only the exact words and concepts provided in the input query.\n\n2-Preserve company names exactly as they appear.\n\n3-Eliminate unnecessary words while retaining the financial focus for searchability.\n\n4-Include relevant Solr fields: company_name, fiscal_year, fiscal_quarter (e.g., \"Q1\"), report_type (e.g., \"results\", \"annual_report\"), or financial_metric (e.g., \"revenue\", \"eps\").\n\n5-If the query includes temporal details (e.g., year, quarter), map them to fiscal_year and fiscal_quarter.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.126613Z","iopub.execute_input":"2025-05-28T09:43:20.127277Z","iopub.status.idle":"2025-05-28T09:43:20.130393Z","shell.execute_reply.started":"2025-05-28T09:43:20.127258Z","shell.execute_reply":"2025-05-28T09:43:20.129892Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def formatting_prompts_func(examples, instruction):\n    inputs = [item.lower() for item in examples['natural_lang_queries']]\n    outputs = [item.lower() for item in examples['solr_queries']]\n    instructions = len(inputs) * [instruction]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = query_rewrite_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.131182Z","iopub.execute_input":"2025-05-28T09:43:20.131392Z","iopub.status.idle":"2025-05-28T09:43:20.148206Z","shell.execute_reply.started":"2025-05-28T09:43:20.131369Z","shell.execute_reply":"2025-05-28T09:43:20.147458Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"cols_to_remove = dataset.column_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.148922Z","iopub.execute_input":"2025-05-28T09:43:20.149214Z","iopub.status.idle":"2025-05-28T09:43:20.161307Z","shell.execute_reply.started":"2025-05-28T09:43:20.149198Z","shell.execute_reply":"2025-05-28T09:43:20.160718Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"dataset = dataset.map(\n    formatting_prompts_func,\n    batched=True,\n    fn_kwargs={\"instruction\": instruction_to_llm}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.162018Z","iopub.execute_input":"2025-05-28T09:43:20.162238Z","iopub.status.idle":"2025-05-28T09:43:20.212169Z","shell.execute_reply.started":"2025-05-28T09:43:20.162223Z","shell.execute_reply":"2025-05-28T09:43:20.211480Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20521e7cf7e04bb5927d83b8db2ebbe6"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"dataset = dataset.remove_columns(column_names=cols_to_remove)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.212839Z","iopub.execute_input":"2025-05-28T09:43:20.213285Z","iopub.status.idle":"2025-05-28T09:43:20.218080Z","shell.execute_reply.started":"2025-05-28T09:43:20.213268Z","shell.execute_reply":"2025-05-28T09:43:20.217547Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)\ndataset[\"validation\"] = dataset[\"test\"]\ndel dataset[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.218644Z","iopub.execute_input":"2025-05-28T09:43:20.218834Z","iopub.status.idle":"2025-05-28T09:43:20.240706Z","shell.execute_reply.started":"2025-05-28T09:43:20.218820Z","shell.execute_reply":"2025-05-28T09:43:20.239960Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.241363Z","iopub.execute_input":"2025-05-28T09:43:20.241637Z","iopub.status.idle":"2025-05-28T09:43:20.250912Z","shell.execute_reply.started":"2025-05-28T09:43:20.241612Z","shell.execute_reply":"2025-05-28T09:43:20.250230Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 900\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 100\n    })\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print(dataset['train'][0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.251770Z","iopub.execute_input":"2025-05-28T09:43:20.252348Z","iopub.status.idle":"2025-05-28T09:43:20.263984Z","shell.execute_reply.started":"2025-05-28T09:43:20.252325Z","shell.execute_reply":"2025-05-28T09:43:20.263280Z"}},"outputs":[{"name":"stdout","text":" \nTask: Rewrite the natural language queries about company financial performance into concise, search-engine-friendly Solr queries.\n\n### Instruction:\n\n1-Use only the exact words and concepts provided in the input query.\n\n2-Preserve company names exactly as they appear.\n\n3-Eliminate unnecessary words while retaining the financial focus for searchability.\n\n4-Include relevant Solr fields: company_name, fiscal_year, fiscal_quarter (e.g., \"Q1\"), report_type (e.g., \"results\", \"annual_report\"), or financial_metric (e.g., \"revenue\", \"eps\").\n\n5-If the query includes temporal details (e.g., year, quarter), map them to fiscal_year and fiscal_quarter.\n\n\n\n### Input:\nwhat are berger paints's revenue for q2 2025?\n\n### Response:\ncompany_name:\"berger paints\" and fiscal_year:2025 and fiscal_quarter:\"q2\" and financial_metric:revenue\n\n<|endoftext|>\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset['train'],\n    # eval_dataset = dataset['validation'],\n    args = TrainingArguments(\n        per_device_train_batch_size = 8,\n        # per_device_eval_batch_size = 8,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # eval_steps = 10,\n        num_train_epochs=1,\n        learning_rate = 1e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 5,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        # output_dir = checkpoint_dir,\n        report_to = \"none\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:20.264804Z","iopub.execute_input":"2025-05-28T09:43:20.265542Z","iopub.status.idle":"2025-05-28T09:43:22.737997Z","shell.execute_reply.started":"2025-05-28T09:43:20.265517Z","shell.execute_reply":"2025-05-28T09:43:22.737333Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/4115173004.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML (num_proc=4):   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d684aa5d28649ffad7593f5427eed85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=4):   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8abbec079949a3b6d0abfea8eef1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=4):   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b496139541d4b38b5ed1d6b5ebd7b2d"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# steps per epoch = train_len(900)/(per_device_train_batch_size(8) * gradient_accumulation_steps(4)) = 28","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:22.738980Z","iopub.execute_input":"2025-05-28T09:43:22.739221Z","iopub.status.idle":"2025-05-28T09:43:22.742829Z","shell.execute_reply.started":"2025-05-28T09:43:22.739198Z","shell.execute_reply":"2025-05-28T09:43:22.742256Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\n\ngpu_stats = torch.cuda.get_device_properties(0)\ntotal_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)  # Total memory in GB\n\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\nstart_reserved_memory = round(torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024, 3)\n\nprint(f\"GPU = {gpu_stats.name}\")\nprint(f\"Total GPU memory: {total_memory} GB\")\nprint(f\"Memory available for training: {total_memory - start_reserved_memory:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:22.743689Z","iopub.execute_input":"2025-05-28T09:43:22.743967Z","iopub.status.idle":"2025-05-28T09:43:22.764647Z","shell.execute_reply.started":"2025-05-28T09:43:22.743944Z","shell.execute_reply":"2025-05-28T09:43:22.764041Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla P100-PCIE-16GB\nTotal GPU memory: 15.888 GB\nMemory available for training: 11.99 GB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:22.765335Z","iopub.execute_input":"2025-05-28T09:43:22.765616Z","iopub.status.idle":"2025-05-28T09:56:45.311350Z","shell.execute_reply.started":"2025-05-28T09:43:22.765591Z","shell.execute_reply":"2025-05-28T09:56:45.310746Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 900 | Num Epochs = 1 | Total steps = 28\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n \"-____-\"     Trainable parameters = 59,768,832/4,000,000,000 (1.49% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 12:47, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>1.621200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.518000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.104300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.079000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.070900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"trainer_stats.metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:56:45.312111Z","iopub.execute_input":"2025-05-28T09:56:45.312407Z","iopub.status.idle":"2025-05-28T09:56:45.317251Z","shell.execute_reply.started":"2025-05-28T09:56:45.312388Z","shell.execute_reply":"2025-05-28T09:56:45.316638Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'train_runtime': 800.2402,\n 'train_samples_per_second': 1.125,\n 'train_steps_per_second': 0.035,\n 'total_flos': 5268659158056960.0,\n 'train_loss': 0.43468867082680973}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"peak_reserved_memory = round(torch.cuda.max_memory_reserved(0) / 1024 / 1024 / 1024, 3)\nused_memory_for_training = round(peak_reserved_memory - start_reserved_memory, 3)\n\nprint(f\"Memory used for training: {used_memory_for_training} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:56:45.317918Z","iopub.execute_input":"2025-05-28T09:56:45.318154Z","iopub.status.idle":"2025-05-28T09:56:45.335899Z","shell.execute_reply.started":"2025-05-28T09:56:45.318133Z","shell.execute_reply":"2025-05-28T09:56:45.335254Z"}},"outputs":[{"name":"stdout","text":"Memory used for training: 0.338 GB\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:56:45.336703Z","iopub.execute_input":"2025-05-28T09:56:45.336935Z","iopub.status.idle":"2025-05-28T09:56:45.366526Z","shell.execute_reply.started":"2025-05-28T09:56:45.336913Z","shell.execute_reply":"2025-05-28T09:56:45.365831Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def get_llm_solr_query(query):\n    inputs = tokenizer(\n    [\n        query_rewrite_prompt.format(\n            instruction_to_llm, # instruction\n            query, # input\n            \"\", # output - leave this blank for generation\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n    \n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n    response = (tokenizer.batch_decode(outputs)[0])\n\n    # print(response)\n    indx = response.find('### Response:')\n    \n    if indx != -1:\n        result_section = response[indx+len('### Response:'):]\n        indx_eos = result_section.find('<|endoftext|>')\n        return result_section[:indx_eos].strip()\n    \n    return response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:56:45.367232Z","iopub.execute_input":"2025-05-28T09:56:45.367484Z","iopub.status.idle":"2025-05-28T09:56:45.380490Z","shell.execute_reply.started":"2025-05-28T09:56:45.367463Z","shell.execute_reply":"2025-05-28T09:56:45.379739Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"query = \"Show TCS earnings per share for Q3 2024\"\nresult = get_llm_solr_query(query)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:56:45.381188Z","iopub.execute_input":"2025-05-28T09:56:45.381438Z","iopub.status.idle":"2025-05-28T09:56:47.743787Z","shell.execute_reply.started":"2025-05-28T09:56:45.381399Z","shell.execute_reply":"2025-05-28T09:56:47.743169Z"}},"outputs":[{"name":"stdout","text":"company_name:\"tcs\" and fiscal_year:2024 and fiscal_quarter:\"q3\" and financial_metric:eps\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"query = \"Wipro's annual report for 2024\"\nresult = get_llm_solr_query(query)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:56:47.746777Z","iopub.execute_input":"2025-05-28T09:56:47.746978Z","iopub.status.idle":"2025-05-28T09:56:49.733978Z","shell.execute_reply.started":"2025-05-28T09:56:47.746963Z","shell.execute_reply":"2025-05-28T09:56:49.733316Z"}},"outputs":[{"name":"stdout","text":"company_name:\"wipro\" and fiscal_year:2024 and report_type:annual_report\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}